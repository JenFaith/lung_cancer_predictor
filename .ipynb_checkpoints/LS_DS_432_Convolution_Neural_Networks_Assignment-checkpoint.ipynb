{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc4yMj7mtCAZ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
    "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
    "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
    "\n",
    "\n",
    "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "|Mountain (+)|Forest (-)|\n",
    "|---|---|\n",
    "|![](https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data/train/mountain/art1131.jpg?raw=1)|![](https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data/validation/forest/cdmc317.jpg?raw=1)|\n",
    "\n",
    "The problem is relatively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several different possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eawBP-otCAb"
   },
   "source": [
    "# Pre - Trained Model\n",
    "<a id=\"p1\"></a>\n",
    "\n",
    "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "```\n",
    "\n",
    "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
    "\n",
    "```python\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "\n",
    "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
    "\n",
    "```python\n",
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(resnet.input, predictions)\n",
    "```\n",
    "\n",
    "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "Steps to complete assignment: \n",
    "1. Load in Image Data into numpy arrays (`X`) \n",
    "2. Create a `y` for the labels\n",
    "3. Train your model with pre-trained layers from resnet\n",
    "4. Report your model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLdGdXCatCAb"
   },
   "source": [
    "## Load in Data\n",
    "\n",
    "This surprisingly more difficult than it seems, because you are working with directories of images instead of a single file. This boiler plate will help you download a zipped version of the directory of images. The directory is organized into \"train\" and \"validation\" which you can use inside an `ImageGenerator` class to stream batches of images thru your model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moRVuHUqtCAc"
   },
   "source": [
    "### Download & Summarize the Data\n",
    "\n",
    "This step is completed for you. Just run the cells and review the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "AR66H8o9tCAc",
    "outputId": "b7c293db-28c1-4b0b-f5ae-25be00f11ec5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "_URL = 'https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data.zip?raw=true'\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file('./data.zip', origin=_URL, extract=True)\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MNFsIu_KtCAg"
   },
   "outputs": [],
   "source": [
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsI9BQLotCAj"
   },
   "outputs": [],
   "source": [
    "train_mountain_dir = os.path.join(train_dir, 'mountain')  # directory with our training cat pictures\n",
    "train_forest_dir = os.path.join(train_dir, 'forest')  # directory with our training dog pictures\n",
    "validation_mountain_dir = os.path.join(validation_dir, 'mountain')  # directory with our validation cat pictures\n",
    "validation_forest_dir = os.path.join(validation_dir, 'forest')  # directory with our validation dog pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUs1e5-XtCAl"
   },
   "outputs": [],
   "source": [
    "num_mountain_tr = len(os.listdir(train_mountain_dir))\n",
    "num_forest_tr = len(os.listdir(train_forest_dir))\n",
    "\n",
    "num_mountain_val = len(os.listdir(validation_mountain_dir))\n",
    "num_forest_val = len(os.listdir(validation_forest_dir))\n",
    "\n",
    "total_train = num_mountain_tr + num_forest_tr\n",
    "total_val = num_mountain_val + num_forest_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "ZmklbgSMtCAn",
    "outputId": "be5b9d52-d9f5-4b1e-878e-eb5f87d0c6bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training mountain images: 254\n",
      "total training forest images: 270\n",
      "total validation mountain images: 125\n",
      "total validation forest images: 62\n",
      "--\n",
      "Total training images: 524\n",
      "Total validation images: 187\n"
     ]
    }
   ],
   "source": [
    "print('total training mountain images:', num_mountain_tr)\n",
    "print('total training forest images:', num_forest_tr)\n",
    "\n",
    "print('total validation mountain images:', num_mountain_val)\n",
    "print('total validation forest images:', num_forest_val)\n",
    "print(\"--\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ4ag4ultCAq"
   },
   "source": [
    "### Keras `ImageGenerator` to Process the Data\n",
    "\n",
    "This step is completed for you, but please review the code. The `ImageGenerator` class reads in batches of data from a directory and pass them to the model one batch at a time. Just like large text files, this method is advantageous, because it stifles the need to load a bunch of images into memory. \n",
    "\n",
    "Check out the documentation for this class method: [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class). You'll expand it's use in the third assignment objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67i9IW49tCAq"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 50\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jenniferquigley/.keras/datasets/./data/train'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1wNKMo1tCAt"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ndsuM4L9tCAv",
    "outputId": "68bd83da-a370-4f6a-a257-d8c6782e1e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 533 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kxlk3optCAy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                              directory=validation_dir,\n",
    "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.DirectoryIterator at 0x14f504e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l7ue6NutCA0"
   },
   "source": [
    "## Instatiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKNIYOEItCA0"
   },
   "outputs": [],
   "source": [
    "resnet = ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(resnet.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.01), \n",
    "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVPBWYG7tCA2"
   },
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4XdvWA5tCA3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.8534 - accuracy: 0.5179 - val_loss: 0.6767 - val_accuracy: 0.6534\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 60s 2s/step - loss: 0.7862 - accuracy: 0.4835 - val_loss: 0.9558 - val_accuracy: 0.6591\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.8877 - accuracy: 0.4977 - val_loss: 0.6595 - val_accuracy: 0.6250\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.6827 - accuracy: 0.5574 - val_loss: 0.6312 - val_accuracy: 0.6534\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.7271 - accuracy: 0.5038 - val_loss: 0.7153 - val_accuracy: 0.4261\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 67s 2s/step - loss: 0.6869 - accuracy: 0.5517 - val_loss: 0.7195 - val_accuracy: 0.3636\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.6907 - accuracy: 0.5916 - val_loss: 0.6224 - val_accuracy: 0.7102\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.6700 - accuracy: 0.6238 - val_loss: 0.6553 - val_accuracy: 0.6420\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 75s 2s/step - loss: 0.6658 - accuracy: 0.5909 - val_loss: 0.7190 - val_accuracy: 0.3636\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 0.6512 - accuracy: 0.5907 - val_loss: 0.6983 - val_accuracy: 0.4886\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.6486 - accuracy: 0.6118 - val_loss: 0.7249 - val_accuracy: 0.3580\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.6663 - accuracy: 0.5609 - val_loss: 0.6404 - val_accuracy: 0.7102\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.6638 - accuracy: 0.5592 - val_loss: 0.6042 - val_accuracy: 0.7102\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 0.6333 - accuracy: 0.6510 - val_loss: 0.7950 - val_accuracy: 0.3409\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 69s 2s/step - loss: 0.6413 - accuracy: 0.6503 - val_loss: 0.5832 - val_accuracy: 0.6761\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.6287 - accuracy: 0.6487 - val_loss: 0.6289 - val_accuracy: 0.7273\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 68s 2s/step - loss: 0.6186 - accuracy: 0.6716 - val_loss: 0.6076 - val_accuracy: 0.7500\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.6126 - accuracy: 0.6651 - val_loss: 0.5797 - val_accuracy: 0.7045\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 67s 2s/step - loss: 0.6081 - accuracy: 0.6760 - val_loss: 0.9458 - val_accuracy: 0.3523\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.6477 - accuracy: 0.6307 - val_loss: 0.6489 - val_accuracy: 0.6648\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.5910 - accuracy: 0.7031 - val_loss: 0.5639 - val_accuracy: 0.7102\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.6586 - accuracy: 0.5998 - val_loss: 0.5733 - val_accuracy: 0.7159\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 60s 2s/step - loss: 0.5899 - accuracy: 0.7280 - val_loss: 0.5619 - val_accuracy: 0.7500\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.6178 - accuracy: 0.6851 - val_loss: 0.6911 - val_accuracy: 0.4943\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 67s 2s/step - loss: 0.6165 - accuracy: 0.6499 - val_loss: 0.5653 - val_accuracy: 0.6761\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6117 - accuracy: 0.6477 - val_loss: 0.5731 - val_accuracy: 0.6648\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6194 - accuracy: 0.6384 - val_loss: 0.6303 - val_accuracy: 0.6761\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5687 - accuracy: 0.7573 - val_loss: 0.8774 - val_accuracy: 0.3636\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.6368 - accuracy: 0.6360 - val_loss: 0.5716 - val_accuracy: 0.7443\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5746 - accuracy: 0.7180 - val_loss: 0.6288 - val_accuracy: 0.6875\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5759 - accuracy: 0.7202 - val_loss: 0.5740 - val_accuracy: 0.7500\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5672 - accuracy: 0.7267 - val_loss: 0.6810 - val_accuracy: 0.5284\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5345 - accuracy: 0.7409 - val_loss: 0.8078 - val_accuracy: 0.4034\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5817 - accuracy: 0.6613 - val_loss: 0.5455 - val_accuracy: 0.6875\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5691 - accuracy: 0.6961 - val_loss: 0.7030 - val_accuracy: 0.5114\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5357 - accuracy: 0.7530 - val_loss: 0.5066 - val_accuracy: 0.7443\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5674 - accuracy: 0.7085 - val_loss: 0.5390 - val_accuracy: 0.7898\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5810 - accuracy: 0.7046 - val_loss: 0.6669 - val_accuracy: 0.5625\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5545 - accuracy: 0.6722 - val_loss: 1.0214 - val_accuracy: 0.3523\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5781 - accuracy: 0.6819 - val_loss: 0.5094 - val_accuracy: 0.7330\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5920 - accuracy: 0.6545 - val_loss: 0.6953 - val_accuracy: 0.5057\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5595 - accuracy: 0.7018 - val_loss: 0.5097 - val_accuracy: 0.6989\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5066 - accuracy: 0.7648 - val_loss: 0.6562 - val_accuracy: 0.6761\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5598 - accuracy: 0.7292 - val_loss: 0.6543 - val_accuracy: 0.5682\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5714 - accuracy: 0.7076 - val_loss: 0.5388 - val_accuracy: 0.7784\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5127 - accuracy: 0.7673 - val_loss: 0.5649 - val_accuracy: 0.6818\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6112 - accuracy: 0.6469 - val_loss: 0.5010 - val_accuracy: 0.7841\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5665 - accuracy: 0.6789 - val_loss: 0.5159 - val_accuracy: 0.7898\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5193 - accuracy: 0.7352 - val_loss: 0.7210 - val_accuracy: 0.4886\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5326 - accuracy: 0.7074 - val_loss: 0.6213 - val_accuracy: 0.6761\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPzsgS94tCA5"
   },
   "source": [
    "# Custom CNN Model\n",
    "\n",
    "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telling python where to find files\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined parameters\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing images\n",
    "# This is also where you would give parameters about rotations, reflections, etc...\n",
    "# range of rgb values is 0 to 255\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 182 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                              directory=validation_dir,\n",
    "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnbJJie3tCA5"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# random value that will get updated during Gradient Descent just like weights in the FCFF network \n",
    "n_weight_matrices = 32\n",
    "\n",
    "# specify the window size (i.e. 3 cell high and 3 cell wide)\n",
    "weight_matrix_size = (3,3)\n",
    "\n",
    "act_func = 'relu'\n",
    "\n",
    "# dim of the image: 32 cell high, 32 cell wide, and 3 channels (one for Red, one for Blue, and one for Green)\n",
    "# this means our images are not matrices, they are tensors \n",
    "image_dim = (32,32,3)\n",
    "\n",
    "pool_size = (2,2)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st conv layer \n",
    "model.add(Conv2D(n_weight_matrices, \n",
    "                 weight_matrix_size, \n",
    "                 activation=act_func, \n",
    "                 input_shape=image_dim))\n",
    "\n",
    "# 1st pooling layer \n",
    "model.add(MaxPooling2D(pool_size))\n",
    "\n",
    "# 2st conv layer \n",
    "model.add(Conv2D(64, weight_matrix_size, activation=act_func))\n",
    "\n",
    "# 2st pooling layer \n",
    "model.add(MaxPooling2D(pool_size))\n",
    "\n",
    "# 3st conv layer \n",
    "model.add(Conv2D(62, weight_matrix_size, activation=act_func))\n",
    "\n",
    "# not adding a 3rd pooling layer becaue the size of the image at this point\n",
    "# is already very small, i.e. (4,4)\n",
    "\n",
    "# flatten the image matrix into a row vector \n",
    "model.add(Flatten())\n",
    "\n",
    "# hidden layer in FCFF portion of model \n",
    "model.add(Dense(64, activation=act_func))\n",
    "\n",
    "# Output layer \n",
    "model.add(Dense(n_features, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P_mRtoutCA9"
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='BinaryCrossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CwM4GsaetCA_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.5553 - accuracy: 0.7024 - val_loss: 0.5083 - val_accuracy: 0.8068\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 71s 2s/step - loss: 0.5391 - accuracy: 0.7500 - val_loss: 0.5933 - val_accuracy: 0.7159\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 68s 2s/step - loss: 0.5382 - accuracy: 0.7401 - val_loss: 0.9435 - val_accuracy: 0.3807\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 75s 2s/step - loss: 0.5303 - accuracy: 0.7381 - val_loss: 0.6117 - val_accuracy: 0.6875\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 75s 2s/step - loss: 0.4945 - accuracy: 0.7599 - val_loss: 0.8879 - val_accuracy: 0.4034\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 76s 2s/step - loss: 0.5354 - accuracy: 0.7560 - val_loss: 0.4545 - val_accuracy: 0.7955\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.5237 - accuracy: 0.7242 - val_loss: 1.5787 - val_accuracy: 0.3352\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.5282 - accuracy: 0.7520 - val_loss: 0.4956 - val_accuracy: 0.8239\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 66s 2s/step - loss: 0.4966 - accuracy: 0.7321 - val_loss: 0.5332 - val_accuracy: 0.7784\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.5541 - accuracy: 0.7222 - val_loss: 0.5299 - val_accuracy: 0.7898\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.5556 - accuracy: 0.7024 - val_loss: 0.4484 - val_accuracy: 0.8068\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5816 - accuracy: 0.6468 - val_loss: 0.4598 - val_accuracy: 0.7841\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 59s 2s/step - loss: 0.5171 - accuracy: 0.7222 - val_loss: 0.4377 - val_accuracy: 0.8182\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5184 - accuracy: 0.7421 - val_loss: 0.5970 - val_accuracy: 0.6591\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5292 - accuracy: 0.7123 - val_loss: 0.8308 - val_accuracy: 0.4261\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.5237 - accuracy: 0.7341 - val_loss: 0.4897 - val_accuracy: 0.8295\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 59s 2s/step - loss: 0.5505 - accuracy: 0.6964 - val_loss: 0.4639 - val_accuracy: 0.8239\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.5203 - accuracy: 0.7440 - val_loss: 0.4724 - val_accuracy: 0.7273\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 61s 2s/step - loss: 0.5060 - accuracy: 0.7540 - val_loss: 0.4331 - val_accuracy: 0.8295\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 65s 2s/step - loss: 0.5782 - accuracy: 0.7024 - val_loss: 0.4650 - val_accuracy: 0.8182\n"
     ]
    }
   ],
   "source": [
    "# Fit Model\n",
    "model1 = model.fit(\n",
    "    train_data_gen,\n",
    "    # Why?\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       loss  accuracy  val_loss  val_accuracy  epoch\n",
      "0  0.816188  0.512974  0.676661      0.653409      0\n",
      "1  0.793887  0.489022  0.955809      0.659091      1\n",
      "2  0.788049  0.510978  0.659528      0.625000      2\n",
      "3  0.696923  0.550898  0.631179      0.653409      3\n",
      "4  0.706367  0.524950  0.715286      0.426136      4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "hist_df= pd.DataFrame(history.history)\n",
    "hist_df['epoch']= hist_df.index.values\n",
    "print(hist_df.head())\n",
    "\n",
    "sns.lineplot(x = 'epoch', y = 'val_accuracy' , data = hist_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 17s - loss: 0.4638 - accuracy: 0.8242\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(val_data_gen, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNTHjUddtCBB"
   },
   "source": [
    "# Custom CNN Model with Image Manipulations\n",
    "\n",
    "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Simply, you should be able to modify our image generator for the problem. Check out these resources to help you get started: \n",
    "\n",
    "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
    "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKioBv3WtCBB"
   },
   "outputs": [],
   "source": [
    "train_datagen2 = ImageDataGenerator(\n",
    "        featurewise_center=True, \n",
    "        featurewise_std_normalization=True,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen2 = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen2 = train_datagen2.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen2 = test_datagen2.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.5229 - accuracy: 0.7183 - val_loss: 0.5477 - val_accuracy: 0.7784\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5140 - accuracy: 0.7619 - val_loss: 0.4516 - val_accuracy: 0.8523\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 63s 2s/step - loss: 0.5494 - accuracy: 0.7421 - val_loss: 0.4182 - val_accuracy: 0.7955\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 64s 2s/step - loss: 0.5177 - accuracy: 0.7282 - val_loss: 0.5322 - val_accuracy: 0.6818\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 62s 2s/step - loss: 0.5590 - accuracy: 0.7222 - val_loss: 0.4287 - val_accuracy: 0.8580\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5715 - accuracy: 0.6865 - val_loss: 0.4276 - val_accuracy: 0.8693\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5445 - accuracy: 0.7083 - val_loss: 0.4086 - val_accuracy: 0.8523\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5886 - accuracy: 0.7163 - val_loss: 0.4521 - val_accuracy: 0.7670\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.4968 - accuracy: 0.7738 - val_loss: 0.4624 - val_accuracy: 0.8295\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.5392 - accuracy: 0.7188 - val_loss: 0.4588 - val_accuracy: 0.7216\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.5102 - accuracy: 0.7540 - val_loss: 0.4065 - val_accuracy: 0.7614\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5738 - accuracy: 0.6964 - val_loss: 0.4805 - val_accuracy: 0.7273\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5307 - accuracy: 0.7361 - val_loss: 0.6581 - val_accuracy: 0.6023\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5067 - accuracy: 0.7778 - val_loss: 0.4352 - val_accuracy: 0.7443\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5394 - accuracy: 0.7222 - val_loss: 0.4078 - val_accuracy: 0.8466\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.4578 - accuracy: 0.7837 - val_loss: 0.4542 - val_accuracy: 0.8352\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 78s 2s/step - loss: 0.5761 - accuracy: 0.6766 - val_loss: 0.4085 - val_accuracy: 0.8750\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.4857 - accuracy: 0.7698 - val_loss: 0.4289 - val_accuracy: 0.8295\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.5616 - accuracy: 0.7044 - val_loss: 0.4223 - val_accuracy: 0.7500\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.5362 - accuracy: 0.7123 - val_loss: 0.3896 - val_accuracy: 0.8693\n"
     ]
    }
   ],
   "source": [
    "# Using same predefined model as above\n",
    "\n",
    "model2 = model.fit(\n",
    "    train_data_gen2,\n",
    "    # Why?\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 18s - loss: 0.3880 - accuracy: 0.8681\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(val_data_gen, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "# Resources and Stretch Goals\n",
    "\n",
    "Stretch goals\n",
    "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
    "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
    "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
    "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
    "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
    "\n",
    "Resources\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
    "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
    "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
    "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
    "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "nteract": {
   "version": "0.23.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
